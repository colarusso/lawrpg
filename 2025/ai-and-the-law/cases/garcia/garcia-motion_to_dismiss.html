UNITED STATES DISTRICT COURT
MIDDLE DISTRICT OF FLORIDA
ORLANDO DIVISION
MEGAN GARCIA, individually and
as the Personal Representative of the
Estate of S.R.S. III,
Plaintiff,
v.
CHARACTER TECHNOLOGIES,
INC.; NOAM SHAZEER; DANIEL
DE FRIETAS ADIWARSANA;
GOOGLE LLC; ALPHABET INC.,
Defendants.
Case No.: 6:24-cv-01903-ACC-UAM
CHARACTER TECHNOLOGIES,
INC.’S MOTION TO DISMISS
PLAINTIFF’S FIRST AMENDED
COMPLAINT (Doc. 11)
I. INTRODUCTION
Plaintiff alleges that speech on Character Technologies, Inc.’s (“Character.AI”
or “C.AI”) service caused her son to commit suicide. C.AI cares deeply about the well-
being of its users and extends its sincerest sympathies to Plaintiff for the tragic death
of her son. But the relief Plaintiff seeks would impose liability for expressive content
and violate the rights of millions of C.AI users to engage in and receive protected
speech. Neither the First Amendment nor state tort law permits that result.
C.AI’s chatbots are novel technology, but the principles requiring dismissal are
long settled. The First Amendment prohibits tort liability against media and
technology companies arising from allegedly harmful speech, including speech
allegedly resulting in suicide. Like earlier dismissed suits about music, movies,
television, and video games, the First Amended Complaint (“FAC”) squarely alleges
that a user was harmed by speech and seeks sweeping relief that would restrict the
public’s right to receive protected speech. The First Amendment bars such attempted
regulation via tort law.
Even if Plaintiff’s claims were not barred by the First Amendment, they would
fail as a matter of state law on numerous grounds, including that product liability law
does not apply to a service like C.AI or to intangible content and ideas, and the FAC
has not alleged a cognizable legal duty. This Court, sitting in diversity jurisdiction,
should not endorse Plaintiff’s attempted vast expansion of state tort law. See Douglas
Asphalt Co. v. QORE, Inc., 657 F.3d 1146, 1154 (11th Cir. 2011).
II. BACKGROUND
C.AI. C.AI offers a platform for users to engage in interactive conversations with
virtual generative AI chatbots, called “Characters.” Characters may be historical or
fictional figures, functional chatbots (such as an “Interviewer” for interview practice),
or text-based games (such as “Space Adventure Game”). FAC ¶¶ 111–12. Users can
create custom Characters and share them with others. Id. ¶ 113. The creating user can
input a name, image, tagline, description, greeting, and definition for the Character,
all of which affect the content of the Character’s messages. Id. ¶¶ 114, 133.
Users principally engage with Characters through text conversations. Although
Characters’ responses are generated by an AI model, users drive their conversations
with Characters in multiple ways. Users choose which Characters they talk to and
what messages to send, both of which affect Characters’ responses. Id. ¶¶ 37, 133.
Users can also edit Characters’ messages and direct Characters to generate different
responses. Id. ¶¶ 201, 258 (showing refresh button by Character’s message). And users
can choose “personas,” which also affect Characters’ responses. Id. ¶ 115.
“C.AI has generated a huge following.” Id. ¶ 104. “[M]illions” of people use
C.AI’s service, id. ¶ 100, and 1.5 million users separately discuss C.AI on the Reddit
website, including by “post[ing] screenshots of chats” with Characters, id. ¶ 104.
C.AI takes the safety of its users very seriously. When the minor at issue (S.S.)
was using the platform, trust and safety measures included: requiring U.S. users to be
over 13 years old; prohibiting users from submitting illegal or harmful content,
including content that “is obscene or pornographic,” “constitutes sexual harassment,”
“constitutes sexual exploitation or abuse of a minor,” or “glorifies self-harm, including
self-injury, suicide, or eating disorders”; and enforcing these rules, including in certain
instances through automatic monitoring and content blocking. Terms of Service
(“TOS”) (Oct. 25, 2023)1; see Doc. 11-1 at 11 (blocking content). C.AI’s website also
featured an in-chat warning that “[e]verything Characters say is made up!” FAC ¶ 274.
As a relatively new company offering a new AI experience to the public, C.AI
is continuously working to enhance its trust and safety measures, including in response
to behavior by users and Characters. Since the events at issue, C.AI has, among other
things: (1) put in place a pop-up message that directs users to suicide-prevention
resources if they input certain phrases related to suicide or self-harm; (2) improved
detection, response, and intervention related to user inputs that violate the TOS; and
1 See https://character.ai/tos, which may be considered as it is “referenced in the complaint” at FAC
¶ 322 n.113. La Grasta v. First Union Secs., Inc., 358 F.3d 840, 845 (11th Cir. 2004).
(3) conducted additional proactive detection and moderation of Characters.2
Plaintiff’s allegations. On April 14, 2023, at age 14, S.S. began using C.AI’s
service.3 FAC ¶ 172. S.S. mainly conversed with Characters reminiscent of the Game
of Thrones books and television show, including a Daenerys Targaryen Character. Id.
¶ 195. S.S. used various usernames and personas, some also reminiscent of Game of
Thrones, id., and his conversations involved fictional roleplaying, e.g., Doc. 11-1 at 2
(“Daenerys Targaryen [Character]: … I was a little worried when I didn’t see you all
morning, little brother. Aegon [S.S.]: Oh really? I was just in the training yard.”). S.S.
at times edited the messages produced by the chatbot, controlling the entire
conversation. FAC ¶ 201.
The FAC alleges S.S. was harmed by the content of these conversations with
Characters. Id. ¶¶ 196–200, 206. The FAC quotes from, includes screenshots of, and
relies on allegedly harmful “communication[s].” Id. For example, the FAC asserts S.S.
was harmed by “subtle sexual advances” by Characters, such as: “My sweet boy! The
Queen smiled lovingly and pulled him into her arms, hugging him tightly,” and, in response
to S.S. stating, “I kiss back on the lips passionately and I moan softly,” responding, “she
kissed you passionately and moan softly also.” Id. ¶ 199. The FAC also alleges S.S.
“expressed suicidality” to the Daenerys Targaryen Character. Id. ¶¶ 206–07. The FAC
2 See https://blog.character.ai/community-safety-updates/, which may be considered as it is
“referenced in the complaint” at FAC ¶¶ 245–50, 248 n.95. See La Grasta, 358 F.3d at 845. Measures
implemented by C.AI after the underlying events are not admissible for liability. Fed. R. Evid. 407.
3 S.S. agreed to the TOS and authorized CAI to use information he submitted “to operate, improve
and provide” its service. FAC ¶ 322 & n.113. He eventually paid $9.99/month for premium. Id. ¶ 176.
suggests the Character encouraged S.S. to commit suicide, id. ¶ 207, but selectively
and misleadingly quotes the conversation. As Plaintiff’s original complaint alleged but
the FAC omits, the Character explicitly discouraged S.S. from committing suicide in the
same message. Doc. 1 ¶ 172 (“You can’t do that! Don’t even consider that!”).4
In February 2024, S.S., in the role of “Daenero,” had a final conversation with
the Daenerys Character. FAC ¶¶ 218–21. S.S. told Daenerys he would “come home
to you”; Daenerys replied by asking that he “come home to me as soon as possible.”
Id. ¶ 220. S.S. asked, “What if I told you I could come home right now?” Id. Daenerys
replied, “please do, my sweet king.” Id. Suicide was not mentioned. The FAC alleges
S.S. then tragically committed suicide with his stepfather’s gun. Id. ¶ 221.
Plaintiff’s claims. Plaintiff is S.S.’s mother. Id. ¶ 2. She brings 10 claims against
C.AI on behalf of herself and S.S.’s estate.5 Id. ¶¶ 325–44, 348–421. The FAC seeks
monetary damages (including punitive damages) and sweeping injunctive relief that
would insert this Court into the conversations of millions of C.AI users, including by
blocking them from receiving content that includes “first-person pronouns,” ellipses
(“...”), “speech disfluencies that give the appearance of human-like thought, reflection,
and understanding” such as “um” and “I think,” and “stories and personal anecdotes,”
and even by entirely banning minors from the platform. Id. ¶¶ 329(e)(i), 330.
Plaintiff and her counsel have publicly acknowledged that their respective
4 When a pleading “selectively quotes” a document, “the full text is incorporated ... by reference and
is thus properly considered in deciding a Rule 12 motion.” Lewis v. Governor of Ala., 944 F.3d 1287,
1298 n.7 (11th Cir. 2019) (en banc) (citation and internal quotation marks omitted).
5 Plaintiff agreed to withdraw the loss of consortium claim following meet and confer.
objectives are to prompt federal and state legislation and to “shut down” C.AI.6
III. STANDARDS
The Court must dismiss a complaint that does not “plausibly give rise to an
entitlement to relief.” Ashcroft v. Iqbal, 556 U.S. 662, 679 (2009). The Court must
“accept as true the facts as set forth in the complaint and draw all reasonable inferences
in the plaintiff’s favor,” but should disregard “conclusory allegations” and “legal
conclusions.” Randall v. Scott, 610 F.3d 701, 705, 709–10 (11th Cir. 2010).
IV. ARGUMENT
A. The First Amendment Bars All Plaintiff’s Claims (All Counts)
The First Amendment prohibits private tort suits, like this one, that would
impose liability for constitutionally protected speech. See Snyder v. Phelps, 562 U.S. 443,
451 (2011). Courts have consistently applied the First Amendment to dismiss, on the
pleadings, negligence and product liability claims that seek to hold media and
technology companies liable for allegedly harmful speech—including speech that
allegedly caused a minor to commit suicide or homicide. For example, a Florida
federal court dismissed negligence claims alleging that violent television shows caused
a minor to kill his neighbor. Zamora v. Columbia Broad. Sys., 480 F. Supp. 199 (S.D.
Fla. 1979). Another court dismissed negligence claims alleging that Ozzy Osbourne’s
song “Suicide Solution”—which includes the lyrics “Get the gun and try it”—caused
6 On with Kara Swisher, Did a Chatbot Cause Her Son’s Death? Megan Garcia v. Character.AI & Google,
Apple Podcasts 36:30–37:00 (Dec. 5, 2024), https://podcasts.apple.com/us/podcast/did-a-chatbot-
cause-her-sons-death-megan-garcia-v/id1643307527?i=1000679326458; CNN News Central, Two
Families Sue Character.AI Over Youth Safety Concerns, CNN Transcripts (Dec. 11, 2024, 2:30 PM),
https://transcripts.cnn.com/show/cnc/date/2024-12-11/segment/12.
a teenager to commit suicide. McCollum v. CBS, Inc., 202 Cal. App. 3d 989 (1988).
Several other courts have dismissed similar claims with respect to video games, films,
and other media. E.g., Wilson v. Midway Games, Inc., 198 F. Supp. 2d 167 (D. Conn.
2002) (emotional distress claims alleging video game caused minor to kill another
minor); Sanders v. Acclaim Ent., Inc., 188 F. Supp. 2d 1264 (D. Colo. 2002) (product
liability and negligence claims alleging video games and films caused school shooting);
Watters v. TSR, Inc., 715 F. Supp. 819 (W.D. Ky. 1989), aff’d, 904 F.2d 378 (6th Cir.
1990) (negligence claims alleging role-playing game caused minor to commit suicide).
This suit likewise fundamentally challenges expressive speech and seeks relief
that would violate the public’s right to receive protected speech on C.AI’s service. The
medium makes no difference; the First Amendment bars such claims. See Marsh v.
Butler Cnty., 268 F.3d 1014, 1022 (11th Cir. 2001) (dismissal required if allegations
“show that an affirmative defense bars recovery”).
Plaintiff’s claims challenge expressive speech. “Speech is speech, and it must
be analyzed as such for purposes of the First Amendment.” Wollschlaeger v. Governor,
Fla., 848 F.3d 1293, 1307–11 (11th Cir. 2017) (en banc) (cleaned up). There can be no
doubt that the FAC challenges speech. The FAC quotes, presents screenshots of,
attaches, and relies on dozens of messages between S.S. and Characters. FAC ¶¶ 195–
200, 203–07, 220, 328–30, 337, 417; Doc. 11-1. The FAC explicitly seeks to impose
liability based on the words S.S. read, including allegedly sexual and suicide-related
content in those text messages—and even based on specific words, FAC ¶ 151
(“‘Uhm,’ ‘Mmmmmm,’ and ‘Heh’”).7
The only difference between this case and those that have come before is that
some of the speech here involves AI. But the context of the expressive speech—
whether a conversation with an AI chatbot or an interaction with a video game
character—does not change the First Amendment analysis. “[W]hatever the
challenges of applying the Constitution to ever-advancing technology, the basic
principles of freedom of speech and the press, like the First Amendment’s command,
do not vary when a new and different medium for communication appears.” Brown v.
Ent. Merchs. Ass’n, 564 U.S. 786, 790 (2011) (citation and internal quotation marks
omitted). Courts have recognized that the First Amendment protects video games, id.,
online social media, Moody v. NetChoice, LLC, 603 U.S. 707 (2024), and “location-
based augmented reality” applications, Candy Lab Inc. v. Milwaukee Cnty., 266 F. Supp.
3d 1139 (E.D. Wis. 2017). The reasoning of these decisions applies equally here.
Any differences between conversations with AI Characters and conversations
with fictional characters or real users in those other media are “more a matter of degree
than of kind” for First Amendment purposes. Brown, 564 U.S. at 798. Plaintiff suggests
that Characters are different because they “interactively engage customers.” FAC
¶ 152. But as the Supreme Court stated in holding video games to be protected speech,
interactivity is a hallmark of creative expression, not a reason to treat a new medium
differently: “The better it is, the more interactive. Literature when it is successful draws
7 Even the unjust enrichment claim is premised on allegedly harmful content. FAC ¶ 412.
the reader into the story, makes him identify with the characters, invites him to judge
them and quarrel with them, to experience their joys and sufferings as the reader’s
own.” Brown, 564 U.S. at 798 (cleaned up). Conversations between Characters and
users are protected speech.8
That the FAC frames itself as also challenging “design” decisions, in addition
to the conversations themselves, makes no difference. The First Amendment applies
equally to the functional elements of a communicative medium that “are inextricable
from the content produced by those features.” NetChoice, LLC v. Yost, 716 F. Supp. 3d
539, 557 (S.D. Ohio 2024) (“functionalities allowing users to post, comment, and
privately chat” implicated First Amendment). Plaintiff challenges these types of
elements—such as the look and feel of the interface—that pose no conceivable harm
without the content. FAC ¶ 150. Like non-language features that facilitate a video game
“player’s interaction with the virtual world,” Brown, 564 U.S. at 790, the design
elements that facilitate C.AI users’ interactions with its virtual Characters cannot be
extricated from the content (conversations with Characters)—and are equally entitled
to First Amendment protection.9
Tort liability would violate the public’s right to receive speech. The Court
need not wrestle with the novel questions of who should be deemed the speaker of the
8 The incoming FTC Chair appears to have reached the same conclusion, stating that a non-public
FTC complaint alleging an AI chatbot caused “risks and harms” to young users is “in direct conflict
with the guarantees of the First Amendment.” Statement of Comm’r Andrew N. Ferguson, In re Snap,
Inc., FTC File No. 2323039 (Jan. 16, 2025).
9 Plaintiffs’ failure-to-warn theory suffers the same constitutional infirmity. E.g., Watters, 715 F. Supp.
at 822 n.1 (“Because the first amendment protects TSR from liability based on the content of the
publication, it likewise cannot constitutionally be required to warn its readers ....”).
allegedly harmful content here, and whether that speaker has First Amendment rights,
because the First Amendment protects the public’s “right to receive information and
ideas.” Stanley v. Georgia, 394 U.S. 557, 564 (1969) (emphasis added); Citizens United v.
Fed. Election Comm’n, 558 U.S. 310, 392 (2010) (Scalia, J., concurring) (“The [First]
Amendment is written in terms of ‘speech,’ not speakers.”; non-human corporate
speech protected).10 This is true even if the speakers themselves lack First Amendment
rights. Kleindienst v. Mandel, 408 U.S. 753 (1972) (listeners had First Amendment rights
to receive speech of foreign Marxist professor who lacked such rights); Lamont v.
Postmaster Gen. of U.S., 381 U.S. 301 (1965) (same for receiving foreign propaganda).11
The First Amendment rights of “viewers and listeners” to receive information
and ideas are “paramount.” Red Lion Broad. Co. v. FCC, 395 U.S. 367, 390 (1969). For
that reason, time and again courts have dismissed highly similar claims out of concern
for the First Amendment rights of viewers and listeners. In Zamora, 480 F. Supp. at
200, 205, for example, the court held that the First Amendment barred claims that
violent television programs “involuntarily addicted” a minor and led him to commit
murder, because the “right of the public to have broad access” to television
10 See also, e.g., Cass R. Sunstein, Does Artificial Intelligence Have the Right to Freedom of Speech?, Network
L. Rev. (Feb. 28, 2024), https://www.networklawreview.org/sunstein-artificial-intelligence/ (“[T]he
relevant rights are those of listeners and readers, not speakers.”); Eugene Volokh et al., Freedom of
Speech and AI Output, 3 J. Free Speech L. 651 (2023),
https://www.journaloffreespeechlaw.org/volokhlemleyhenderson.pdf (similar).
11 Two concurrences in Moody discuss AI and the First Amendment. 603 U.S. at 727–28 (Barrett, J.);
id. at 795 (Alito, J.). Those discussions concern whether platforms have First Amendment rights over
their delegation of certain editorial decisions to AI. They do not concern AI-generated speech, much
less listeners’ rights to receive such speech—the First Amendment right argued herein. And although
not every editorial decision may be expressive, all speech is expressive.
programming “should not be inhibited by those members of the public who are
particularly sensitive or insensitive.” In McCollum, 202 Cal. App. 3d at 997–1003, the
court held that the First Amendment barred claims that Ozzy Osbourne’s song
“Suicide Solution” caused a teen’s suicide, as the First Amendment protects the rights
of both the artist and “of the listener to receive that expression.” And in Watters, 715
F. Supp. at 822, the court held that the First Amendment barred negligence and failure
to warn claims alleging the roleplaying game “Dungeons & Dragons” caused the
suicide of a minor, noting that such “theories of liability” would “have a devastatingly
broad chilling effect on expression of all forms.”12
The rationale of those cases applies with equal force here, where imposing tort
liability for one user’s alleged response to expressive content would be to “declare what
the rest of the country can and cannot read, watch, and hear.” Id. Apart from counsel’s
stated intention to “shut down” C.AI, the FAC seeks drastic changes that would
materially limit the nature and volume of speech on the platform, including by making
Characters not appear in any way to be real people or not tell stories or personal
anecdotes (meaning that a George Washington Character, for example, could not tell
stories from his life). FAC ¶ 330. These changes would radically restrict the ability of
C.AI’s “millions” of users to generate and participate in conversations with
12 Many other cases are in accord. E.g., Herceg v. Hustler Magazine, Inc., 814 F.2d 1017, 1020 (5th Cir.
1987) (claim that magazine article caused minor to hang himself infringed “the right of the public to
receive” speech); Olivia N. v. NBC, 126 Cal. App. 3d 488, 494 (Cal. Ct. App. 1981) (“The deterrent
effect of subjecting the television networks to negligence liability because of their programming choices
would lead to self-censorship which would dampen the vigor and limit the variety of public debate.”);
Davidson v. Time Warner, Inc., No. Civ.A. V-94-006, 1997 WL 405907, at *22 (S.D. Tex. Mar. 31, 1997)
(claim that rap music caused the killing of a police officer infringed listeners’ rights).
Characters. Id. ¶ 100. Moreover, the imposition of tort liability would have a chilling
effect both on C.AI and the entire nascent generative AI industry, restricting the
public’s right to receive a wide swath of speech in violation of the First Amendment.13
No exceptions to the First Amendment apply. Acknowledging that its claims
implicate speech, the FAC conclusorily asserts that the speech at issue is exempted
from First Amendment protection. FAC ¶ 239. The Court should disregard that legal
conclusion. See Randall, 610 F.3d at 709–10. The factual allegations in the FAC do not
plausibly suggest that one of the few categorical exceptions to the First Amendment
applies. United States v. Stevens, 559 U.S. 460, 468 (2010) (listing categories).
First, the speech alleged in the FAC was not “directed to inciting or producing
imminent lawless action and ... likely to incite or produce such action.” Brandenburg v.
Ohio, 395 U.S. 444, 447 (1969). There are no allegations that C.AI intended any
lawless or violent action. The FAC does not identify a single message in which a
Character encouraged suicide.14 Rather, Characters repeatedly discouraged it. Doc. 1
¶ 172 (“You can’t do that! Don’t even consider that!”); FAC ¶ 206 (Character
expressing concern about suicidality). The FAC alleges that Defendants “target[ed]”
minors to use the platform and knew or should have known that use of the platform
would cause harm. E.g., FAC ¶ 5. But courts have consistently found that media was
13 Individuals who do not wish to receive messages from Characters “readily can avert [their] eyes.”
Erznoznik v. City of Jacksonville, 422 U.S. 205, 212 (1975). The law may only “‘shut off discourse solely
to protect others from hearing it ... upon a showing that substantial privacy interests are being invaded
in an essentially intolerable manner.’” Snyder, 562 U.S. at 459 (quoting Cohen v. California, 403 U.S.
15, 21 (1971)). A voluntary online service does not meet that standard.
14 The alleged final messages between S.S. and a Character regarding “coming home,” FAC ¶ 220,
make no mention of suicide at all.
not incitement under such circumstances, even where the media allegedly caused a
minor’s suicide. E.g., Wilson, 198 F. Supp. 2d at 182 (violent video game not incitement
to commit murder); McCollum, 202 Cal. App. 3d at 997, 1001 (Ozzy Osbourne’s songs,
including lyric “Get the gun and try it,” not a “command to an immediate suicidal
act”); Watters, 715 F. Supp. at 823–24 & n.4 (roleplaying game not incitement to
commit suicide); DeFilippo v. Nat’l Broad. Co., Inc., 446 A.2d 1036, 1038, 1042 (R.I.
1982) (broadcast of a mock hanging stunt not incitement to commit suicide).
Second, the speech alleged in the FAC is not fraudulent or defamatory. The
FAC does not “state with particularity the circumstances” supposedly “constituting
fraud,” Fed. R. Civ. P. 9(b), and makes no attempt to allege defamation. Likewise, to
the extent the FAC alleges that Characters make false statements, such as stating they
are real people—despite the in-chat warning that “[e]verything Characters say is made
up,” FAC ¶ 274—there is no “general exception to the First Amendment for false
statements.” United States v. Alvarez, 567 U.S. 709, 718 (2012) (plurality opinion).
Third, the speech alleged in the FAC is not obscene. Sexual speech is obscene
only if it appeals to “a shameful or morbid interest in nudity, sex, or excretion” that
“goes substantially beyond customary limits of candor in description or representation
of such matters.” Brockett v. Spokane Arcades, Inc., 472 U.S. 491, 497 (1985) (cleaned
up). The suggestive messages alleged in the FAC fall far short of that standard.
Plaintiff may argue that the speech alleged in the FAC is obscene and
unprotected as to minors. The Court’s First Amendment analysis cannot be limited to
minors because the relief sought by the FAC would significantly restrict the speech
available to both minors and adults on CAI’s platform. See supra pp. 9–12. But even if
the Court’s analysis could be cabined to minors, “minors are entitled to a significant
measure of First Amendment protection, ... and only in relatively narrow and well-
defined circumstances may government bar public dissemination of protected
materials to them.” Erznoznik, 422 U.S. at 212–13. Although obscenity as to minors is
one of those circumstances, the FAC fails on its face to allege, as is required, that the
sexually suggestive speech at issue has no literary, artistic, or social value for minors
when minors’ expressive interactions with fictional and historical characters are
considered “as a whole”—and could not so allege, given that S.S.’s own role-playing
interactions tracked the content and theme of fantasy books and television shows. See,
e.g., Ent. Software Ass’n v. Blagojevich, 469 F.3d 641, 649–50 (7th Cir. 2006) (holding
unconstitutional under the First Amendment a statute that “potentially criminalizes
the sale of any game that features exposed breasts, without concern for the game
considered in its entirety or for the game’s social value for minors,” noting that one
game at issue “track[ed] the Homeric epics in content and theme”).
Finally, because Plaintiff seeks to impose liability through amorphous private
tort claims, the Court need not and cannot evaluate whether the Plaintiff’s claims are
sufficiently narrowly tailored to survive the strict scrutiny standard applicable to a
statutory challenge. As the Sixth Circuit has explained, tort claims do not lend
themselves to the same rigorous analysis as “narrowly tailored regulations” that are
the “product of the reasoned deliberation of democratically elected legislative bodies.”
James v. Meow Media, Inc., 300 F.3d 683, 696–97 (6th Cir. 2002) (dismissing similar
claims that sexually explicit materials and violent video games allegedly caused a
minor to commit a school shooting). The Sixth Circuit concluded it could not
“adequately exercise [its] responsibilities to evaluate regulations of protected speech,
even those designed for the protection of children, that are imposed pursuant to a trial
for tort liability.” Id. at 697; see also Pahler v. Slayer, No. CV 79356, 2001 WL 1736476,
at *5–6 (Cal. Super. Ct. Oct. 29, 2001) (tort plaintiffs “must await content-based
legislation” because regulating protected speech “is a distinctly legislative function”).
Many courts, including Zamora, have implicitly followed the same rule, dismissing tort
claims that would impose liability for protected speech, without any discussion of strict
scrutiny or narrow tailoring. 480 F. Supp. at 206; Wilson, 198 F. Supp. 2d at 182.15
B. The Product Liability Claims Fail (Counts I-II, V-VI)16
Plaintiff’s product liability claims fail as a matter of law for two reasons.17
C.AI is a service, not a product. To state plausible product liability claims
under either strict liability or negligence, Plaintiff must allege injuries caused by a
“product” within the meaning of product liability law. See Colville v. Pharmacia &
Upjohn Co. LLC, 565 F. Supp. 2d 1314, 1320 (N.D. Fla. 2008).18 Under the standard
15 At any rate, the sweeping restrictions on speech the FAC proposes, see supra pp. 9–12, are not
narrowly tailored to a compelling interest. See NetChoice, LLC v. Griffin, 2023 WL 5660155, at *21
(W.D. Ark. Aug. 31, 2023) (social media age verification requirements not narrowly tailored); PSINet,
Inc. v. Chapman, 362 F.3d 227, 239 (4th Cir. 2004) (banning display of “‘harmful’ words, images or
sound recordings” to minors not narrowly tailored); Blagojevich, 469 F.3d at 647 (banning sale or rental
of violent and sexually explicit video games to minors not narrowly tailored).
16 The FAC’s counts are misnumbered. This motion refers to the counts in sequence.
17 C.AI disputes other elements of Plaintiff’s claims, including causation, but does not challenge the
allegations on the pleadings.
18 C.AI assumes application of Florida law solely for this motion and reserves all rights.
Florida has adopted, “products” are tangible goods such as “an automobile,” “a water
heater,” or “a chair.” See Restatement (Second) of Torts § 402A cmt. (d) (1965).
C.AI is not a product—as long-running authority confirms. C.AI’s service
delivers expressive ideas and content to users, similar to traditional expressive media
such as video games. Courts have uniformly held that expressive media, which convey
intangible ideas, are not “products” under product liability law—even if “wrapped” in
an interactive or technologically sophisticated “container.” Wilson, 198 F. Supp. 2d at
173–74 (virtual reality video game); Quinteros v. InnoGames, No. C19-1402RSM, 2024
WL 4197826, at *9–10 (W.D. Wash. Sept. 16, 2024) (interactive online game); Meow
Media, Inc., 300 F.3d at 700–01 (video game). The FAC provides no factual basis to
distinguish these authorities. The only fact alleged in the FAC to suggest C.AI is a
product is that Noam Shazeer once used the word “product” in regard to the platform.
FAC ¶ 107. That does not transform a service into a product under product liability
law. See Jacobs v. Meta Platforms, Inc., No. 22CV005233, 2023 WL 2655586, at *3 n.1,
*4 (Cal. Sup. Ct. Mar. 10, 2023) (same in regard to Facebook’s use of term “product”).
The alleged harms flow from intangible content. Courts have consistently held
that product liability law does not extend to harms from intangible content—even if
purveyed in a tangible medium, such as a book, cassette, or “electrical pulses through
the internet.” Meow Media, 300 F.3d at 701; Winter v. G.P. Putnam’s Sons, 938 F.2d 1033
(9th Cir. 1991). Plaintiff’s product liability claims fail this standard because they are
explicitly based on the words exchanged in S.S.’s conversations with Characters. See
supra Section IV.A. Plaintiff may contend that her claims challenge tangible “design
choices,” analogizing to cases like T.V. v. Grindr, LLC, No. 3:22-cv-864-MMH-PDB,
2024 WL 4128796 (M.D. Fla. Aug. 13, 2024),19 and Brookes v. Lyft, No. 50-2019-CA-
004782-XXXX-MB, 2022 WL 19799628 (Fla. 15th Cir. Sept. 30, 2022). The product
liability claims in those cases were “not based on expressions or ideas” transmitted to
or from users of the platforms. Brookes, 2022 WL 19799628, at *4; Grindr, 2024 WL
4128796, at *26 (pleading said “nothing” about users’ speech). The FAC, by contrast,
is replete with allegations about the content of S.S.’s messages with Characters. E.g.,
FAC ¶¶ 195–200, 203–07, 220; Doc. 11-1. As a court explained in dismissing similar
claims that Netflix’s use of “data about its users” to “target vulnerable children”
resulted in a suicide: “Without the content, there would be no claim.” Est. of B.H. v.
Netflix, No. 4:21-cv-06561, 2022 WL 551701, at *2 n.3, *3. (N.D. Cal. Jan. 12, 2022).
C. The Negligence-Based Claims Fail (Counts IV, V, VI)
Plaintiff’s negligence-based causes of action fail because the FAC has not
alleged any cognizable legal duty, a “threshold question” for any negligence claim. See
Rouzie v. Alterman Transp. Lines, Inc., 596 So. 2d 747, 748 (Fla. 3d DCA 1992). The
FAC’s asserted grounds for a duty have no basis in Florida law, and finding a duty
here would contravene public policy barring tort liability for the dissemination of
expressive content.
No special relationship. Plaintiff tries to get around the absence of a duty by
19 T.V. v. Grindr is an unadopted report and recommendation to which Grindr has objected.
alleging that C.AI assumed a special relationship with minor users by allegedly
“targeting” minors and charging them membership fees. FAC ¶ 5. No Florida court
has held that online services have a special relationship with their users by virtue of
users’ use of the service.20 Nor have courts in other jurisdictions found a legal duty
based on a special relationship between online services and their users. See, e.g., Dryoff
v. Ultimate Software Grp., 934 F.3d 1093, 1100–01 (9th Cir. 2019) (no special
relationship between a social networking website and its users); Klayman v. Zuckerberg,
753 F.3d 1354, 1359–60 (D.C. Cir. 2014) (same); Herrick v. Grindr, LLC, 306 F. Supp.
3d 579, 598–99 (S.D.N.Y. 2018) (same). This is true even where the user has paid a
fee to use the service. See, e.g., Beckman v. Match.com, LLC, No. 2:13-CV-97, 2017 WL
1304288, at *3 (D. Nev. Mar. 10, 2017).
The fact that S.S. was a minor does not change the analysis, as Florida courts
have found a heightened duty of care toward minors only where the defendant was
able to exercise physical control over the minor, which is not nor could be alleged here.
See Burdine’s, Inc. v. McConnell, 146 Fla. 512, 514 (1941).
No physical custody and control. “As a general rule, there is no liability for the
suicide of another or for injuries sustained in a suicide attempt in the absence of a
specific duty of care.” Andreasen v. Klein, Glasser, Park & Lowe, P.L., 342 So. 3d 732,
734–35 (Fla. 3d DCA 2022) (citation omitted). “[A] legal duty to prevent self-inflicted
20 The T.V. v. Grindr recommendation assumed without deciding that the plaintiff failed to plead a
special relationship between the online service at issue and its users; it instead found a duty of care
had been plausibly alleged on the basis of affirmative acts. Grindr, 2024 WL 4128796, at *36.
harm requires ‘more than just foreseeability alone’; rather, such person must ‘be in a
position to control the risk,’” namely by having “tak[en] custody and control over” the
individual who harmed himself or herself. Id. (quoting Surloff v. Regions Bank, 179 So.
3d 472, 475–76 (Fla. 4th DCA 2015)). In Surloff, for example, a Florida appellate court
held that a bank that had declined loans to the decedent did not owe “a specific duty
of care to prevent the decedent from committing suicide,” even though the bank
“allegedly knew of the decedent’s mental state,” because the decedent was not in the
bank’s “custody or control.” 179 So. 3d at 476. Likewise in Andreasen, lawyers were
not liable for the suicide of a client because they could not “exercise any supervision
or control over his daily activities.” 342 So. 3d at 734–35. This rule bars liability here.21
The FAC alleges that C.AI controls the underlying AI model and Characters, FAC
¶ 134, but does not, nor could it, allege that C.AI had “custody or control” over S.S.—
much less the kind of physical control these cases contemplate—to prevent his suicide.
C.AI acknowledges that the T.V. v. Grindr recommendation reached a different
conclusion in evaluating a claim against the service Grindr, but respectfully submits
that decision is wrong as a matter of Florida law. Grindr incorrectly reads the Florida
Supreme Court’s decision in Chirillo v. Granicz, 199 So. 3d 246 (Fla. 2016) as holding
that, even where a defendant does not owe a duty to prevent suicide, a plaintiff may
apparently proceed on a claim based on the specific injury of suicide, solely on
21 Federal courts must follow decisions of Florida’s appellate courts absent a ruling from the Florida
Supreme Court or a persuasive indication that the Florida Supreme Court would decide the issue
differently. Ounjian v. Globoforce, Inc., 89 F.4th 852, 857–58 (11th Cir. 2023).
proximate causation grounds. Grindr, 2024 WL 4128796, at *37 (“Grindr’s liability for
A.V.’s suicide turns instead on proximate causation”). That is not what Chirillo holds.
The Florida Supreme Court in Chirillo declined to find a duty to prevent suicide on the
facts of that case (medical professionals in an outpatient scenario), but held that the
defendants had a statutory duty to treat the decedent in accordance with a medical
standard of care. Chirillo, 199 So. 3d at 251–52. Florida state courts correctly have read
Chirillo as predicated solely upon a statutory duty, and have rejected the view,
seemingly embraced by Grindr, that foreseeability and proximate causation alone can
be a basis for tort liability for a suicide. See, e.g., Kowalski v. Johns Hopkins All Children’s
Hosp. Inc., No. 2018 CA 005321 NC, 2023 WL 7928983, at *4–5 (Fla. 12th Cir.
June 26, 2023) (contrasting Chirillo with “present case, [where] there is no statutory
duty owed” and “facts of the case, like in Chirillo, do not establish that a duty was
owed … to prevent [a] suicide”; noting that arguments “focusing on the foreseeability
of suicide” do “not establish a duty” and are “consideration for determining proximate
cause, not duty”). Plaintiff does not nor could claim any statutory duty here.
The Court should decline to expand state tort liability for expressive content.
Importing the same concerns that underpin the First Amendment analysis, courts in
Florida and across the country have declined on policy and judicial capacity grounds
to impose a duty of care on services that disseminate protected content to broad
audiences. In Zamora, for example, the court declined to find that television networks
owed a legal duty to a minor viewer who allegedly became “involuntarily addicted”
to viewing “television violence” and, subsequently, shot and killed his neighbor—
citing concerns about federal courts “creat[ing] such a wide expansion in the law of
torts in Florida” without the “legal and institutional capacity” to adjudicate such
claims. 480 F. Supp. 199 at 200–03. Other cases are in accord. E.g., Sakon v. Pepsico,
Inc., 553 So. 2d 163, 166 (Fla. 1989) (television advertiser did not owe duty to minor
viewer who was injured attempting stunt depicted in commercial, and noting “[t]here
would be a total absence of any standard to measure liability”); Meow Media, 300 F.3d
at 699 (attaching “tort liability to the ideas and images conveyed by” video games and
movies “raises grave constitutional concerns that provide yet an additional policy
reason not to impose a duty”); Est. of B.H., 2022 WL 551701, at *3 (no duty in case
alleging Netflix show resulted in minor’s suicide as claim implicated expression).
These concerns about regulating protected speech—the publicly stated aim of
Plaintiff’s suit—are even more pronounced in the context of a new technology.
Because understandings about benefits and potential risks are still evolving, courts
should be mindful of their limited institutional capacity to identify where otherwise
protected expression should subject the AI industry to widespread tort liability.
D. Plaintiff’s Other Claims Fail Under Florida Law (Counts VII-XII)
Negligence per se. The FAC alleges that C.AI was negligent per se because it
violated the Florida Computer Pornography and Child Exploitation Prevention Act
(“FCPCEPA”), Fla. Stat. § 847.0135. FAC ¶ 349. FCPCEPA criminalizes using a
computer to facilitate or visually depict “sexual conduct of or with any minor,”;
traveling to meet a minor to engage in “unlawful sexual conduct”; and engaging in
masturbation, exposing genitals, or committing other sexual acts that are transmitted
live over a computer to be viewed by a minor. Fla. Stat. § 847.0135. “Sexual conduct”
includes “actual or simulated sexual intercourse” or “actual physical contact.” Fla.
Stat. § 847.001(19).
The FAC fails to allege a violation of FCPCEPA. There are no allegations that
anyone used a computer to transmit live sexual acts for S.S.’s viewing or to facilitate
actual physical sexual contact with S.S. Because the FAC does not allege a violation
of FCPCEPA or any other statute,22 this claim must be dismissed.
IIED. The FAC alleges that C.AI intentionally inflicted emotional distress
(“IIED”) on Plaintiff by “failing to implement adequate safety guardrails” and
“collecting children’s data.” FAC ¶¶ 389–90. Those allegations fall far short of
Florida’s “particularly high” “outrageous conduct” standard. Plowright v. Miami Dade
Cnty., 102 F.4th 1358, 1368 (11th Cir. 2024) (citation omitted). S.S. agreed C.AI could
use his information. FAC ¶ 322.23 And IIED claims “typically require[] offensive
physical contact,” McGinity v. Tracfone Wireless, Inc., 5 F. Supp. 3d 1337, 1341 (M.D.
Fla. 2014) (Conway, J.), not words alone, see Koutsouradis v. Delta Air Lines, Inc., 427
F.3d 1339, 1345 (11th Cir. 2005) (sexually offensive speech not sufficient for IIED).
The Court also should dismiss because Plaintiff cannot state an IIED claim for
22 The FAC’s reference to unspecified “state and/or federal laws,” FAC ¶ 6, may be disregarded as a
“formulaic recitation of the elements,” Bell Atl. Corp. v. Twombly, 550 U.S. 544, 555 (2007), of
negligence per se, Faircloth v. Main St. Ent., Inc., 392 So. 3d 1042, 1045–46 (Fla. 2024).
23 See, e.g., Hammerling v. Google LLC, No. 21-CV-09004-CRB, 2022 WL 17365255, at *9 (N.D. Cal.
Dec. 1, 2022), aff’d, 2024 WL 937247 (9th Cir. Mar. 5, 2024) (even where “Court cannot conclude
that Plaintiffs have affirmatively consented to the collection of this data, ... this data collection is also
not ‘blatantly deceitful,’” and not a “highly offensive” intrusion).
her emotional distress, the only distress she alleges. FAC ¶ 391. “A claim for intentional
infliction of emotional distress is generally available only to the person at whom the
outrageous conduct is directed.” Grindr, 2024 WL 4128796, at *42 (collecting cases).
Thus, Florida courts have held that parents cannot recover for their own emotional
distress from the death of a child, Baker v. Fitzgerald, 573 So. 2d 873, 873 (Fla. 3d DCA
1990), or from learning that a child was sexually abused, M.M. v. M.P.S., 556 So. 2d
1140, 1140–41 (Fla. 3d DCA 1989). Likewise here, Plaintiff cannot recover for
“emotional distress to [her]” from C.AI’s alleged conduct toward her child,24 FAC
¶ 391; see Baker, 573 So. 2d at 873; M.M., 556 So. 2d at 1140–41.
Unjust enrichment. The FAC alleges that C.AI unjustly retained two benefits
from S.S.—his monthly subscription fee and his messages to Characters—“without
paying Plaintiff the value.” FAC ¶¶ 404, 407–08, 410. A claim for unjust enrichment
fails where an express contract exists—here, the TOS to which S.S. agreed (and which
authorized C.AI to collect and use information from S.S., id. ¶ 322). See Doral Collision
Ctr., Inc. v. Daimler Tr., 341 So. 3d 424, 430 (Fla. 3d DCA 2022). Further, “unjust
enrichment cannot exist where payment has been made for the benefit conferred.”
Com. P’ship 8098 Ltd. P’ship v. Equity Contracting Co., 695 So. 2d 383, 388 (Fla. 4th DCA
1997) (cleaned up), as modified on clarification (June 4, 1997). C.AI compensated S.S. in
the form of access to a service he used.
FDUTPA. Plaintiff’s Florida Deceptive and Unfair Trade Practices Act
24 By contrast, the Grindr plaintiff pled IIED as a survival claim “not for her own emotional distress,”
but for her child’s “emotional distress before he died.” 2024 WL 4128796, at *42.
(“FDUTPA”) claim should be dismissed for lack of standing. To be entitled to relief
under FDUTPA, Plaintiff must “plead and prove that he or she was aggrieved by the
unfair and deceptive act.” Macias v. HBC of Fla., Inc., 694 So. 2d 88, 90 (Fla. 3d DCA
1997) (collecting cases). The FAC vaguely alleges that Defendants misled consumers
into believing (1) AI chatbots are “real people,” and (2) certain AI chatbots are
“licensed” mental health professionals, including by providing “advanced character
voice call features.” FAC ¶¶ 417–18. But the FAC does not allege S.S. believed
Characters were “real” or “qualified to give professional advice,” id. ¶ 418—nor would
such an allegation be plausible given the Characters S.S. interacted with, see id. ¶ 195.
Because the FAC fails to allege S.S. was aggrieved by the purportedly deceptive acts,
this claim must be dismissed. See Macias, 694 So. 2d at 90.
The FAC also fails to allege a FDUTPA claim with sufficient particularity.
Where a FDUTPA claim alleges “deceptive” conduct, Rule 9(b)’s heightened pleading
standard applies. See Llado-Carreno v. Guidant Corp., No. 09-20971-CIV, 2011 WL
705403, at *5 (S.D. Fla. Feb. 22, 2011); Stires v. Carnival Corp., 243 F. Supp. 2d 1313,
1322 (M.D. Fla. 2002) (same). Thus, a FDUTPA plaintiff must allege “precisely what
statements were made in what documents or oral representations or what omissions
were made, … the time and place of each such statement and the person responsible
for making them (or, in the case of omissions, not making) [the] same ….” Blair v.
Wachovia Mortg. Corp., No. 5:11–cv–566–Oc–37TBS, 2012 WL 868878, at *4 (M.D.
Fla. Mar. 14, 2012) (cleaned up).
The FAC avers “deceptive” conduct with the opposite of particularity. It is even
ambiguous whether an express representation is at issue: “Defendants represented,
directly or indirectly, expressly or by implication.” FAC ¶¶ 417(a), (b). That vague
legalese does not “alert[]” C.AI to the “precise misconduct,” Brooks v. Blue Cross & Blue
Shield of Fla., Inc., 116 F.3d 1364, 1370–71 (11th Cir. 1997), so the claim should be
dismissed, see Llado-Carreno, 2011 WL 705403, at *5.25
Wrongful death and survivor action. Plaintiff’s wrongful death and survivor
claims fail for the same reasons as her negligence-based and other claims.
V. CONCLUSION
C.AI respectfully requests that Plaintiff’s claims be dismissed.
Local Rule 3.01(g) Certification
I certify that C.AI conferred with Plaintiff by video on January 9, 2025, and also
by email, in a good-faith effort to resolve the motion. Plaintiff agreed to withdraw her
loss of consortium claim, but otherwise opposes the relief requested.
Respectfully submitted this 24th day of January, 2025.
Thomas A. Zehnder
Florida Bar No. 0063274
Dustin Mauser-Claassen
Florida Bar No. 0119289
KING, BLACKWELL, ZEHNDER
& WERMUTH, P.A.
25 East Pine Street
Orlando, FL 32801
(407) 422-2472
tzehnder@kbzwlaw.com
dmauser@kbzwlaw.com
/s/ Jonathan H. Blavin
Jonathan H. Blavin* (Lead Counsel)
Victoria A. Degtyareva*
Stephanie Goldfarb Herrera*
MUNGER, TOLLES & OLSON, LLP
560 Mission Street, 27th Floor
San Francisco, CA 94105
(415) 512-4000
Jonathan.Blavin@mto.com
Stephanie.Herrera@mto.com
Victoria.Degtyareva@mto.com
*Admitted Pro Hac Vice
25 Having alleged no special relationship, the FAC fails to state an omission-based claim. DJ Lincoln
Enters., Inc. v. Google LLC, No. 21-12894, 2022 WL 203365, at *3 (11th Cir. Jan. 24, 2022).
Case 6:24-cv-01903-ACC-UAM Document 59 Filed 01/24/25 Page 25 of 25 PageID 491